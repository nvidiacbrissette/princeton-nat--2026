{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Surge of Agents with the NeMo Agent Toolkit: Integrating Multiple AI Frameworks\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Welcome to this hands-on tutorial on integrating multiple AI frameworks with the NeMo Agent Toolkit! In this notebook, we'll transform the \"Surge of Agents\" example into an NeMo Agent Toolkit workflow that orchestrates four powerful frameworks:\n",
    "\n",
    "- **OpenAI Python Library**: For direct access to language models with a clean API\n",
    "- **LangChain**: For structured data handling and composable processing chains\n",
    "- **LangGraph**: For graph-based workflow management with modular components\n",
    "- **CrewAI**: For collaborative multi-agent orchestration with specialized roles\n",
    "\n",
    "The key insight of this tutorial is that **you don't need to rewrite your existing code** to benefit from the NeMo Agent Toolkit's orchestration capabilities. Instead, we'll create thin wrapper components that integrate your framework-specific code into a unified workflow.\n",
    "\n",
    "By the end of this notebook, you'll understand how to:\n",
    "1. Create NeMo Agent Toolkit components that wrap existing framework code\n",
    "2. Configure a workflow that connects these components\n",
    "3. Run the workflow as a unified system\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Before diving into the frameworks, we need to establish our development environment. We'll configure access to the NVIDIA AI Foundation Models platform, which provides access to powerful open-source models like Llama 3.1.\n",
    "\n",
    "### Key Configuration Elements:\n",
    "\n",
    "- **API Key**: The authentication token required to access NVIDIA's API services. In production environments, this should be stored securely as an environment variable rather than hardcoded in your notebooks.\n",
    "\n",
    "- **Model Selection**: We're using `meta/llama-3.1-70b-instruct`, a powerful open-source LLM that balances performance and efficiency contained in an NVIDIA NIM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by setting up these configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\nfrom dotenv import find_dotenv, load_dotenv\n\nload_dotenv(find_dotenv())\n\nmodel_name = \"meta/llama-3.1-70b-instruct\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Project Setup\n",
    "\n",
    "Now we'll create the project structure for our NeMo Agent Toolkit workflow. We'll use the NeMo Agent Toolkit CLI to create a new workflow and set up the necessary directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the workflows directory if it doesn't exist\n",
    "!mkdir -p workflows\n",
    "\n",
    "# Create a new NeMo Agent Toolkit workflow\n",
    "!nat workflow create --no-install --workflow-dir workflows surge_of_agents\n",
    "\n",
    "# Create additional directories for configs and data\n",
    "!mkdir -p workflows/surge_of_agents/configs\n",
    "!mkdir -p workflows/surge_of_agents/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the project structure that was created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the project structure\n",
    "!tree workflows/surge_of_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NeMo Agent Toolkit CLI has created a basic project structure with:\n",
    "- `pyproject.toml`: Package configuration file\n",
    "- `src/surge_of_agents/`: Source directory for our components\n",
    "  - `__init__.py`: Package initialization file\n",
    "  - `register.py`: Component registration file\n",
    "  - `surge_of_agents_function.py`: Default NeMo Agent Toolkit component file\n",
    "\n",
    "Now, let's update the `pyproject.toml` file to include the dependencies we need for our multi-framework integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/surge_of_agents/pyproject.toml\n",
    "[build-system]\n",
    "build-backend = \"setuptools.build_meta\"\n",
    "requires = [\"setuptools >= 64\"]\n",
    "\n",
    "[project]\n",
    "name = \"surge_of_agents\"\n",
    "version = \"0.1.0\"\n",
    "dependencies = [\n",
    "  \"nvidia-nat[langchain,llama-index,crewai]\",\n",
    "  \"langchain_nvidia_ai_endpoints\",\n",
    "  \"pydantic\",\n",
    "]\n",
    "requires-python = \">=3.12\"\n",
    "description = \"NeMo Agent Toolkit workflow integrating multiple AI frameworks\"\n",
    "classifiers = [\"Programming Language :: Python\"]\n",
    "\n",
    "[project.entry-points.\"nat.components\"]\n",
    "surge_of_agents = \"surge_of_agents.register\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Framework Component Wrappers\n",
    "\n",
    "Now we'll create wrapper components for each of the four frameworks. These wrappers will allow us to integrate existing framework-specific code into our NeMo Agent Toolkit workflow without rewriting it.\n",
    "\n",
    "### 1. OpenAI Wrapper Component\n",
    "\n",
    "First, let's create a wrapper for the OpenAI Python library that generates math equations. This component will:\n",
    "1. Use the OpenAI client to access the NVIDIA API\n",
    "2. Generate a math equation suitable for pre-algebra students\n",
    "3. Return the equation as output\n",
    "\n",
    "Let's create this component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/surge_of_agents/src/surge_of_agents/openai_wrapper.py\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EquationGeneratorConfig(FunctionBaseConfig, name=\"equation_generator\"):\n",
    "    \"\"\"Configuration for the equation generator component.\"\"\"\n",
    "    model_name: str = \"meta/llama-3.1-70b-instruct\"\n",
    "    temperature: float = 0.5\n",
    "\n",
    "@register_function(config_type=EquationGeneratorConfig)\n",
    "async def equation_generator(config: EquationGeneratorConfig, builder: Builder):\n",
    "    \"\"\"\n",
    "    A wrapper for the OpenAI Python library that generates math equations.\n",
    "    \"\"\"\n",
    "    from openai import OpenAI\n",
    "    import os\n",
    "\n",
    "    async def _generate_equation(student_level: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a math equation using the OpenAI API.\n",
    "        \n",
    "        Args:\n",
    "            student_level: The student level of the equation, such as \"pre-algebra\", \"algebra\", \"geometry\", or \"calculus\"\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing the generated equation\n",
    "        \"\"\"\n",
    "        # Initialize the OpenAI client\n",
    "        client = OpenAI(\n",
    "            organization=\"nvidia\",\n",
    "            base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "            api_key=os.getenv(\"NVIDIA_API_KEY\"),\n",
    "        )\n",
    "        \n",
    "        # Create the prompt based on student_level\n",
    "        prompt = f\"\"\"\n",
    "        Create a math equation suitable for a {student_level} student that involves solving for a single variable, x.\n",
    "        Provide only the equation, like \"3x - 5 = 10\".\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the equation\n",
    "        response = client.chat.completions.create(\n",
    "            model=config.model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=config.temperature\n",
    "        )\n",
    "        \n",
    "        # Extract and return the equation\n",
    "        equation = response.choices[0].message.content.strip()\n",
    "        return {\"equation\": equation}\n",
    "\n",
    "    yield FunctionInfo.from_fn(\n",
    "        _generate_equation,\n",
    "        description=\"Generates math equations of varying difficulty levels using the OpenAI API\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LangChain Wrapper Component\n",
    "\n",
    "Next, let's create a wrapper for LangChain that generates word problems from equations. This component will:\n",
    "1. Use LangChain's prompt templates and output parsers\n",
    "2. Create a structured workflow using the pipeline operator\n",
    "3. Return a word problem that matches the given equation\n",
    "\n",
    "Let's create this component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/surge_of_agents/src/surge_of_agents/langchain_wrapper.py\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.framework_enum import LLMFrameworkEnum\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.component_ref import LLMRef\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class WordProblemGeneratorConfig(FunctionBaseConfig, name=\"word_problem_generator\"):\n",
    "    \"\"\"Configuration for the word problem generator component.\"\"\"\n",
    "    llm_name: LLMRef\n",
    "    debug_mode: bool = False\n",
    "\n",
    "@register_function(config_type=WordProblemGeneratorConfig, framework_wrappers=[LLMFrameworkEnum.LANGCHAIN])\n",
    "async def word_problem_generator(config: WordProblemGeneratorConfig, builder: Builder):\n",
    "    \"\"\"\n",
    "    A wrapper for LangChain that generates word problems from equations.\n",
    "    \"\"\"\n",
    "    from langchain.globals import set_debug\n",
    "    from langchain.output_parsers import PydanticOutputParser\n",
    "    from langchain_core.prompts import PromptTemplate\n",
    "    from pydantic import BaseModel, Field\n",
    "    \n",
    "    # Enable debug mode if requested\n",
    "    if config.debug_mode:\n",
    "        set_debug(True)\n",
    "    \n",
    "    # Get the LLM from the builder\n",
    "    llm = await builder.get_llm(config.llm_name, wrapper_type=LLMFrameworkEnum.LANGCHAIN)\n",
    "    \n",
    "    # Define a structured data model for word problems\n",
    "    class WordProblem(BaseModel):\n",
    "        word_problem: str = Field(description=\"The text of the math word problem\")\n",
    "    \n",
    "    # Create a parser that will extract structured data from LLM responses\n",
    "    word_problem_parser = PydanticOutputParser(pydantic_object=WordProblem)\n",
    "    \n",
    "    # Define a template for generating word problems with instructions for proper formatting\n",
    "    word_problem_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Given the equation {equation}, create a realistic word problem that matches it.\n",
    "        The problem should involve a real-world scenario (e.g., shopping, travel) and require solving for x.\n",
    "        Provide only the word problem.\n",
    "        Format your response as JSON: {format_instructions}. Do not include any other text but the JSON.\"\"\",\n",
    "        partial_variables={\"format_instructions\": word_problem_parser.get_format_instructions()}\n",
    "    )\n",
    "    \n",
    "    # Compose the entire workflow as a chain using the pipeline operator\n",
    "    chain = word_problem_prompt | llm | word_problem_parser\n",
    "    \n",
    "    async def _generate_word_problem(equation: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a word problem from an equation using LangChain.\n",
    "        \n",
    "        Args:\n",
    "            equation: The math equation to convert into a word problem\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing the equation and generated word problem\n",
    "        \"\"\"\n",
    "        # Execute the chain with the equation\n",
    "        result = await chain.ainvoke({\"equation\": equation})\n",
    "        \n",
    "        # Return the equation and word problem\n",
    "        return {\n",
    "            \"equation\": equation,\n",
    "            \"word_problem\": result.word_problem\n",
    "        }\n",
    "    \n",
    "    yield FunctionInfo.from_fn(\n",
    "        _generate_word_problem,\n",
    "        description=\"Generates word problems from math equations using LangChain\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LangGraph Wrapper Component\n",
    "\n",
    "Now, let's create a wrapper for LangGraph that solves the equation and explains the solution. This component will:\n",
    "1. Define nodes for solving and explaining\n",
    "2. Create a graph with connections between nodes\n",
    "3. Execute the graph to process the equation and word problem\n",
    "\n",
    "Let's create this component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/surge_of_agents/src/surge_of_agents/langgraph_wrapper.py\n",
    "import logging\n",
    "from typing import Dict, Any\n",
    "\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.framework_enum import LLMFrameworkEnum\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.component_ref import LLMRef\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EquationSolverConfig(FunctionBaseConfig, name=\"equation_solver\"):\n",
    "    \"\"\"Configuration for the equation solver component.\"\"\"\n",
    "    llm_name: LLMRef\n",
    "    debug_mode: bool = False\n",
    "\n",
    "@register_function(config_type=EquationSolverConfig, framework_wrappers=[LLMFrameworkEnum.LANGCHAIN])\n",
    "async def equation_solver(config: EquationSolverConfig, builder: Builder):\n",
    "    \"\"\"\n",
    "    A wrapper for LangGraph that solves equations and explains the solutions.\n",
    "    \"\"\"\n",
    "    from langchain.globals import set_debug\n",
    "    from langchain_core.prompts import PromptTemplate\n",
    "    from langgraph.graph import Graph, START\n",
    "    \n",
    "    # Enable debug mode if requested\n",
    "    if config.debug_mode:\n",
    "        set_debug(True)\n",
    "    \n",
    "    # Get the LLM from the builder\n",
    "    llm = await builder.get_llm(config.llm_name, wrapper_type=LLMFrameworkEnum.LANGCHAIN)\n",
    "    \n",
    "    # Create prompts for solving and explaining\n",
    "    equation_solver_prompt = PromptTemplate(\n",
    "        input_variables=[\"equation\", \"word_problem\"],\n",
    "        template=\"\"\"Given the equation {equation} and matching word problem {word_problem}, solve it by providing only the mathematical steps as a list.\n",
    "        Each part should be a single equation or expression, showing the progression to the final solution, without any explanatory text. For example, for \"5 + x = 13\", output:\n",
    "        5 + x = 13 -> x = 13 - 5 -> x = 8\"\"\"\n",
    "    )\n",
    "    \n",
    "    equation_solution_explainer_prompt = PromptTemplate(\n",
    "        input_variables=[\"equation\", \"word_problem\", \"solution\"],\n",
    "        template=\"\"\"Given the equation {equation}, the matching word problem {word_problem}, and the solution {solution}, explain the solution in plain English using the fewest words possible.\"\"\"\n",
    "    )\n",
    "    \n",
    "    # Create chains\n",
    "    equation_solver_chain = equation_solver_prompt | llm\n",
    "    equation_solution_explainer_chain = equation_solution_explainer_prompt | llm\n",
    "    \n",
    "    # Define node functions\n",
    "    def equation_solver_node(input_dict):\n",
    "        equation = input_dict[\"equation\"]\n",
    "        word_problem = input_dict[\"word_problem\"]\n",
    "        solution = equation_solver_chain.invoke({\"equation\": equation, \"word_problem\": word_problem})\n",
    "        # Ensure solution is a string (extract content if it's an AIMessage)\n",
    "        if hasattr(solution, 'content'):\n",
    "            solution = solution.content\n",
    "        return {\"solution\": solution, \"equation\": equation, \"word_problem\": word_problem}\n",
    "    \n",
    "    def equation_solution_explainer_node(input_dict):\n",
    "        equation = input_dict[\"equation\"]\n",
    "        word_problem = input_dict[\"word_problem\"]\n",
    "        solution = input_dict[\"solution\"]\n",
    "        explanation = equation_solution_explainer_chain.invoke({\"equation\": equation, \"word_problem\": word_problem, \"solution\": solution})\n",
    "        # Ensure explanation is a string (extract content if it's an AIMessage)\n",
    "        if hasattr(explanation, 'content'):\n",
    "            explanation = explanation.content\n",
    "        return {\"explanation\": explanation, \"equation\": equation, \"word_problem\": word_problem, \"solution\": solution}\n",
    "    \n",
    "    async def _solve_and_explain(equation: str, word_problem: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Solve an equation and explain the solution using LangGraph.\n",
    "        \n",
    "        Args:\n",
    "            equation: The math equation to solve\n",
    "            word_problem: The word problem that matches the equation\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing the equation, word problem, solution, and explanation\n",
    "        \"\"\"\n",
    "        # Create our workflow graph\n",
    "        graph = Graph()\n",
    "        \n",
    "        # Add our processing nodes\n",
    "        graph.add_node(\"Solve Equation\", equation_solver_node)\n",
    "        graph.add_node(\"Explain Solution\", equation_solution_explainer_node)\n",
    "        \n",
    "        # Define the flow between nodes\n",
    "        graph.add_edge(START, \"Solve Equation\")\n",
    "        graph.add_edge(\"Solve Equation\", \"Explain Solution\")\n",
    "        \n",
    "        # Set the finish point to the last node so its output is returned\n",
    "        graph.set_finish_point(\"Explain Solution\")\n",
    "        \n",
    "        # Compile the graph into a runnable workflow\n",
    "        workflow = graph.compile()\n",
    "        \n",
    "        # Run the workflow with our input data\n",
    "        workflow_result = workflow.invoke({\n",
    "            \"equation\": equation,\n",
    "            \"word_problem\": word_problem\n",
    "        })\n",
    "\n",
    "        print(workflow_result)\n",
    "        \n",
    "        # Return the workflow result\n",
    "        return workflow_result\n",
    "    \n",
    "    yield FunctionInfo.from_fn(\n",
    "        _solve_and_explain,\n",
    "        description=\"Solves math equations and provides step-by-step explanations using LangGraph\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. CrewAI Wrapper Component\n",
    "\n",
    "Finally, let's create a wrapper for CrewAI that reviews the solution. This component will:\n",
    "1. Create specialized agents with distinct roles and goals\n",
    "2. Define tasks for each agent to perform\n",
    "3. Coordinate their collaboration through a sequential workflow\n",
    "\n",
    "Let's create this component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/surge_of_agents/src/surge_of_agents/crewai_wrapper.py\n",
    "import logging\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "from nat.builder.builder import Builder\n",
    "from nat.builder.function_info import FunctionInfo\n",
    "from nat.cli.register_workflow import register_function\n",
    "from nat.data_models.function import FunctionBaseConfig\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SolutionReviewerConfig(FunctionBaseConfig, name=\"solution_reviewer\"):\n",
    "    \"\"\"Configuration for the solution reviewer component.\"\"\"\n",
    "    model_name: str = \"meta/llama-3.1-70b-instruct\"\n",
    "    verbose: bool = True\n",
    "\n",
    "@register_function(config_type=SolutionReviewerConfig)\n",
    "async def solution_reviewer(config: SolutionReviewerConfig, builder: Builder):\n",
    "    \"\"\"\n",
    "    A wrapper for CrewAI that reviews math solutions.\n",
    "    \"\"\"\n",
    "    from crewai import Agent, Task, Crew, LLM\n",
    "    import os\n",
    "    \n",
    "    async def _review_solution(equation: str, word_problem: str, solution: str, explanation: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Review a math solution using CrewAI.\n",
    "        \n",
    "        Args:\n",
    "            equation: The math equation\n",
    "            word_problem: The word problem that matches the equation\n",
    "            solution: The step-by-step solution\n",
    "            explanation: The explanation of the solution\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing the review results\n",
    "        \"\"\"\n",
    "        # Initialize the LLM with the correct format for CrewAI\n",
    "        llm = LLM(\n",
    "            model=f\"nvidia_nim/{config.model_name}\", \n",
    "            api_key=os.getenv(\"NVIDIA_API_KEY\")\n",
    "        )\n",
    "        \n",
    "        # Agent 1: Accuracy Checker\n",
    "        accuracy_checker_agent = Agent(\n",
    "            role=\"Accuracy Checker\",\n",
    "            goal=\"Verify the mathematical correctness of the word problem, equation, and solution steps\",\n",
    "            backstory=\"You are a meticulous mathematician with a keen eye for detail. Your expertise lies in ensuring that every calculation and logical step in a math problem is correct, leaving no room for errors. You double-check solutions against the original problem to confirm accuracy.\",\n",
    "            llm=llm,\n",
    "            verbose=config.verbose\n",
    "        )\n",
    "        \n",
    "        # Define the accuracy checking task\n",
    "        accuracy_task = Task(\n",
    "            description=f\"Review the following: word problem '{word_problem}', equation '{equation}', and solution '{solution}'. Verify that the solution steps correctly solve the equation and match the word problem. Output 'Correct' if accurate, or identify any errors if incorrect.\",\n",
    "            expected_output=\"A concise statement confirming accuracy ('Correct') or detailing any errors found.\",\n",
    "            agent=accuracy_checker_agent,\n",
    "        )\n",
    "        \n",
    "        # Agent 2: Clarity Reviewer\n",
    "        clarity_reviewer_agent = Agent(\n",
    "            role=\"Clarity Reviewer\",\n",
    "            goal=\"Ensure the word problem and solution explanation are clear, engaging, and educationally valuable for students\",\n",
    "            backstory=\"You are an experienced educator with a passion for making math accessible and engaging. You excel at evaluating whether problems and explanations are easy to understand, appropriately challenging, and relevant to students' learning needs.\",\n",
    "            llm=llm,\n",
    "            verbose=config.verbose,\n",
    "        )\n",
    "        \n",
    "        # Define the clarity review task\n",
    "        clarity_task = Task(\n",
    "            description=f\"Review the following: word problem '{word_problem}' and solution explanation '{explanation}'. Assess if they are clear, engaging, and suitable for middle school students. Provide feedback, including at least one suggestion for improvement if applicable.\",\n",
    "            expected_output=\"A brief assessment of clarity and educational value, plus one suggestion for enhancement.\",\n",
    "            agent=clarity_reviewer_agent,\n",
    "        )\n",
    "        \n",
    "        # Create a crew with both agents and their tasks\n",
    "        crew = Crew(\n",
    "            agents=[accuracy_checker_agent, clarity_reviewer_agent], \n",
    "            tasks=[accuracy_task, clarity_task], \n",
    "            process=\"sequential\",  # Tasks will be executed in order \n",
    "            verbose=config.verbose\n",
    "        )\n",
    "        \n",
    "        # Execute the full workflow\n",
    "        result = crew.kickoff()\n",
    "        \n",
    "        # Return the review results\n",
    "        return {\n",
    "            \"equation\": equation,\n",
    "            \"word_problem\": word_problem,\n",
    "            \"solution\": solution,\n",
    "            \"explanation\": explanation,\n",
    "            \"review\": result\n",
    "        }\n",
    "    \n",
    "    yield FunctionInfo.from_fn(\n",
    "        _review_solution,\n",
    "        description=\"Reviews math solutions for accuracy and clarity using CrewAI\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Updating the Register File\n",
    "\n",
    "Now that we've created all our wrapper components, we need to update the `register.py` file to import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/surge_of_agents/src/surge_of_agents/register.py\n",
    "# pylint: disable=unused-import\n",
    "# flake8: noqa\n",
    "\n",
    "# Import all wrapper components\n",
    "from surge_of_agents.openai_wrapper import equation_generator\n",
    "from surge_of_agents.langchain_wrapper import word_problem_generator\n",
    "from surge_of_agents.langgraph_wrapper import equation_solver\n",
    "from surge_of_agents.crewai_wrapper import solution_reviewer\n",
    "\n",
    "__all__ = [\n",
    "    \"equation_generator\",\n",
    "    \"word_problem_generator\",\n",
    "    \"equation_solver\",\n",
    "    \"solution_reviewer\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll implement a structured sequential workflow that explicitly manages the data flow between components. This approach ensures proper data passing between tools and provides a clear execution sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Running Our Multiframework Workflow\n",
    "\n",
    "### 1. Installing\n",
    "\n",
    "Now let's install our package and test the sequential workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%uv pip install -e workflows/surge_of_agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the Workflow Configuration\n",
    "\n",
    "Now we'll create a configuration file that defines our sequential workflow. First, create a directory for the config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p workflows/surge_of_agents/configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll write the config file using the various frameworks we wrapped as NeMo Agent Toolkit components above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/surge_of_agents/configs/sequential_config.yml\n",
    "general:\n",
    "  uvloop: true\n",
    "  telemetry:\n",
    "    tracing:\n",
    "      phoenix:\n",
    "          _type: phoenix\n",
    "          endpoint: http://localhost:6006/v1/traces\n",
    "          project: surge_of_agents\n",
    "\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    temperature: 0.7\n",
    "    max_tokens: 1000\n",
    "    \n",
    "functions:\n",
    "  equation_generator:\n",
    "    _type: equation_generator\n",
    "    llm_name: nim_llm\n",
    "    \n",
    "  word_problem_generator:\n",
    "    _type: word_problem_generator\n",
    "    llm_name: nim_llm\n",
    "    \n",
    "  equation_solver:\n",
    "    _type: equation_solver\n",
    "    llm_name: nim_llm\n",
    "\n",
    "  solution_reviewer:\n",
    "    _type: solution_reviewer\n",
    "    llm_name: nim_llm\n",
    "\n",
    "workflow:\n",
    "  _type: react_agent\n",
    "  llm_name: nim_llm\n",
    "  system_prompt: |\n",
    "    You are a helpful assistant that follows a sequential workflow to create and solve math problems.\n",
    "\n",
    "    You need to perform the following steps in order:\n",
    "    1. Generate a math equation\n",
    "    2. Create a word problem based on the equation\n",
    "    3. Solve the equation\n",
    "    4. Review the solution\n",
    "\n",
    "    You have access to the following tools:\n",
    "\n",
    "    {tools}\n",
    "\n",
    "    You may respond in one of two formats.\n",
    "    Use the following format exactly to ask the human to use a tool:\n",
    "\n",
    "    Question: the input question you must answer\n",
    "    Thought: you should always think about what to do\n",
    "    Action: the action to take, should be one of [{tool_names}]\n",
    "    Action Input: the input to the action (if there is no required input, include \"Action Input: None\")  \n",
    "    Observation: wait for the human to respond with the result from the tool, do not assume the response\n",
    "\n",
    "    ... (this Thought/Action/Action Input/Observation can repeat N times. If you do not need to use a tool, or after asking the human to use any tools and waiting for the human to respond, you might know the final answer.)\n",
    "    Use the following format once you have the final answer:\n",
    "\n",
    "    Thought: I now know the final answer\n",
    "    Final Answer: the equation, word problem, solution, and explanation\n",
    "  \n",
    "  tool_names:\n",
    "    - equation_generator\n",
    "    - word_problem_generator\n",
    "    - equation_solver\n",
    "    - solution_reviewer\n",
    "  verbose: true\n",
    "  retry_parsing_errors: true\n",
    "  max_retries: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Starting Phoenix for Observability\n",
    "\n",
    "Now we can start Phoenix and open the Phoenix UI to see the profiling data. We have instrumented our config for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start the Phoenix server using Popen to gain direct control over the process\n",
    "# We also suppress the output by redirecting stdout and stderr\n",
    "phoenix_process = subprocess.Popen(\n",
    "    [\"phoenix\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "\n",
    "# Give Phoenix a moment to start up\n",
    "time.sleep(3)\n",
    "\n",
    "print(f\"Phoenix server started with PID: {phoenix_process.pid}\")\n",
    "print(\"You can access the Phoenix UI at: http://localhost:6006\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the Workflow\n",
    "\n",
    "Now, at long last, we can run our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat run --config_file workflows/surge_of_agents/configs/sequential_config.yml --input \"Create a math problem about for pre-algebra students\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stopping the Phoenix Server\n",
    "\n",
    "Let's clean up by stopping the Phoenix server we started. In a production system, you would not stop Phoenix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the process object exists and is running\n",
    "if 'phoenix_process' in locals() and phoenix_process.poll() is None:\n",
    "    print(f\"Stopping Phoenix server with PID: {phoenix_process.pid}...\")\n",
    "    \n",
    "    # Send the termination signal to the process\n",
    "    phoenix_process.terminate()\n",
    "    \n",
    "    try:\n",
    "        # Wait for the process to terminate\n",
    "        phoenix_process.wait(timeout=5)\n",
    "        print(\"Phoenix server stopped successfully.\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        # If it doesn't terminate gracefully, force kill it\n",
    "        print(\"Server did not terminate gracefully. Forcing kill...\")\n",
    "        phoenix_process.kill()\n",
    "        phoenix_process.wait()\n",
    "        print(\"Phoenix server killed.\")\n",
    "    \n",
    "    # Clean up the variable\n",
    "    del phoenix_process\n",
    "else:\n",
    "    print(\"Phoenix server was not running or the process object was not found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to integrate multiple AI frameworks using the NeMo Agent Toolkit.\n",
    "\n",
    "This approach showcases the benefits of the NeMo Agent Toolkit:\n",
    "\n",
    "- **Framework Interoperability**: Seamlessly integrate OpenAI, LangChain, LangGraph, and CrewAI.\n",
    "- **Workflow Management**: Orchestrate complex workflows with proper data handling.\n",
    "- **Deployment Options**: Deploy workflows as APIs, CLIs, or web applications.\n",
    "- **Extensibility**: Easily add new components or modify existing ones."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}