{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nvidia-branding-header",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deep-dive-framing",
   "metadata": {},
   "source": "> **Deep Dive**: This notebook is part of the deep-dive series that extends [03_Evaluation_Observability_And_Optimization.ipynb](../03_Evaluation_Observability_And_Optimization.ipynb). In the previous notebooks, you were introduced to evaluation, observability, and optimization using a simple math agent. This deep-dive takes those same concepts to production depth using a real-world email phishing analyzer workflow with custom evaluators, advanced profiling, and multi-objective optimization."
  },
  {
   "cell_type": "markdown",
   "id": "4e7493e7-d510-4f9b-bf51-ca9d4fa010df",
   "metadata": {},
   "source": [
    "# Email Phishing Analyzer Optimizer Notebook\n",
    "\n",
    "Welcome to the third leg of the Email Phishing Analyzer journey. After learning how to evaluate and profile the workflow, this notebook-style guide shows you how to *improve* it using the NeMo Agent Toolkit Optimizer. We will follow the reference guide in [`docs/source/reference/optimizer.md`](../../../docs/source/reference/optimizer.md) and make it concrete for this workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06e3bff-004d-4185-8385-c4a658e6ac09",
   "metadata": {},
   "source": [
    "## Orientation & What You Will Learn\n",
    "\n",
    "By the end of this notebook you will know how to:\n",
    "- Discover which workflow components are tunable and how their search spaces are defined.\n",
    "- Understand the optimizer configuration in `config_optimizer.yml`, including multi-objective scoring and prompt GA settings.\n",
    "- Run the optimizer with both numeric and prompt modes engaged.\n",
    "- Inspect optimizer artifacts (`optimized_config.yml`, `ga_history_prompts.csv`, Pareto plots, etc.) and decide what to ship.\n",
    "- Close the loop by validating tuned configs with evaluation and profiling.\n",
    "\n",
    "> **Mindset:** Treat the optimizer as your lab partner. It explores broad parameter spaces quickly, but you direct the experiment by choosing metrics, search spaces, and stopping criteria.\n",
    "\n",
    "## Prerequisites & Environment Warm-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a8abc1-b580-4a51-95ee-08e3ce82dc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the phishing workflow package in editable mode\n",
    "! uv pip install -e .\n",
    "\n",
    "# Confirm the CLI entry point is available\n",
    "! nat --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6143940e-c063-4385-90ab-d539a384b3d5",
   "metadata": {},
   "source": [
    "**Before moving on:**\n",
    "For this notebook, you will need the following API keys to run all examples end-to-end:\n",
    "\n",
    "NVIDIA Build: You can obtain an NVIDIA Build API Key by creating an NVIDIA Build account and generating a key at https://build.nvidia.com/settings/api-keys\n",
    "Then you can run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d49276-f994-4760-9fb6-bf5cb7c883f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"NVIDIA_API_KEY\" not in os.environ:\n",
    "    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311e8fe-c036-47ea-85f0-3237e46aba5b",
   "metadata": {},
   "source": [
    "## Establish Paths & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b011ef-123e-4248-b532-7e7f28a1a4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root = Path.cwd()\n",
    "workflow_dir = root \n",
    "config_opt_path = workflow_dir / \"configs\" / \"config_optimizer.yml\"\n",
    "optimizer_output_dir = Path(\"eval_with_optimizer\")\n",
    "config_opt_path, optimizer_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087d86b-cca9-41f0-972e-fdb60dc4f221",
   "metadata": {},
   "source": [
    "## Meet the Optimizable Fields in Code\n",
    "\n",
    "The optimizer can only tune parameters marked with `OptimizableField`. Let’s confirm which knobs the phishing workflow exposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdbbc19-0295-4c60-b36f-7d82b0253424",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "from nat_email_phishing_analyzer import register as phishing_register\n",
    "\n",
    "for name, cls in inspect.getmembers(phishing_register, inspect.isclass):\n",
    "    if not hasattr(cls, \"model_fields\"):\n",
    "        continue\n",
    "    optimizable = []\n",
    "    for field_name, field_info in cls.model_fields.items():\n",
    "        extras = getattr(field_info, \"json_schema_extra\", {}) or {}\n",
    "        if extras.get(\"optimizable\"):\n",
    "            search_space = extras.get(\"search_space\")\n",
    "            optimizable.append((field_name, search_space))\n",
    "    if optimizable:\n",
    "        print(name)\n",
    "        for field_name, search_space in optimizable:\n",
    "            print(f\"  - {field_name}: search_space={search_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a28fb-9c68-4cb7-b1ba-d76b1be6e9c0",
   "metadata": {},
   "source": [
    "Alternatively, read the source directly:\n",
    "- `sensitive_info_detector`, `intent_classifier`: tunable `llm` and `prompt`.\n",
    "- `link_and_domain_analyzer`: tunable `min_suspicious_score`.\n",
    "- `phishing_risk_aggregator`: tunable explanation LLM, weights, and decision threshold.\n",
    "- LLM configs (`llama_3_405`, `llama_3_70`) expose temperature/top_p/max_tokens.\n",
    "\n",
    "**Key takeaway:** Every tunable parameter inherits from `OptimizableMixin` and declares `optimizable_params` in the YAML. Combine code + config to see the whole search space.\n",
    "\n",
    "While you are exploring the printed search spaces, note which ones are categorical (`values`) versus numeric (`low`/`high`). Categorical knobs typically converge faster but may require broader coverage of options, whereas numeric knobs benefit from more trials so Optuna can narrow in on precise values.\n",
    "\n",
    "## Decode the Optimizer Configuration\n",
    "\n",
    "Open `config_optimizer.yml` and focus on three sections: `functions`, `llms`, and `optimizer`.\n",
    "\n",
    "```yaml\n",
    "# Snippet: optimizer section\n",
    "optimizer:\n",
    "  output_path: eval_output_with_optimizer\n",
    "  reps_per_param_set: 1\n",
    "  eval_metrics:\n",
    "    rag_accuracy:\n",
    "      evaluator_name: rag_accuracy\n",
    "      direction: maximize\n",
    "    rag_groundedness:\n",
    "      evaluator_name: rag_groundedness\n",
    "      direction: maximize\n",
    "    token_efficiency:\n",
    "      evaluator_name: token_efficiency\n",
    "      direction: minimize\n",
    "    latency:\n",
    "      evaluator_name: llm_latency\n",
    "      direction: minimize\n",
    "\n",
    "  numeric:\n",
    "    enabled: true\n",
    "    n_trials: 5\n",
    "\n",
    "  prompt:\n",
    "    enabled: true\n",
    "    prompt_population_init_function: prompt_init\n",
    "    prompt_recombination_function: prompt_recombination\n",
    "    ga_generations: 3\n",
    "    ga_population_size: 3\n",
    "    ga_diversity_lambda: 0.3\n",
    "    ga_parallel_evaluations: 1\n",
    "```\n",
    "\n",
    "**What this tells us:**\n",
    "- Trials use four metrics simultaneously. The optimizer will normalize each (max vs min) and combine them (default `harmonic` unless overridden).\n",
    "- Numeric mode runs Optuna for five trials. Prompt mode runs a three-generation GA with a small population (great for quick demos).\n",
    "- Prompt initialization and recombination functions are defined in the config’s `functions` section so the optimizer knows which LLM (`prompt_optimizer`) to call for mutations.\n",
    "\n",
    "Consider experimenting with `multi_objective_combination_mode` (`harmonic`, `sum`, or `chebyshev`) and metric weights when your priorities shift. Emphasising latency, for example, can push the optimizer toward lighter-weight models even if accuracy dips slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d90505-15b5-468d-bad5-86dd2f5e7452",
   "metadata": {},
   "source": [
    "## Visualize Search Spaces at Runtime\n",
    "\n",
    "Override search spaces dynamically to experiment without editing source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff207138-9c29-4ffe-b4fb-38648c7941fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "cfg = yaml.safe_load(config_opt_path.read_text())\n",
    "\n",
    "# Example: tighten decision threshold range and expand temperature\n",
    "cfg['functions']['phishing_risk_aggregator'].setdefault('search_space', {})['decision_threshold'] = {\n",
    "    'low': 0.4,\n",
    "    'high': 0.7,\n",
    "    'step': 0.05,\n",
    "}\n",
    "cfg['llms']['llama_3_70'].setdefault('search_space', {})['temperature'] = {\n",
    "    'low': 0.0,\n",
    "    'high': 0.8,\n",
    "    'step': 0.1,\n",
    "}\n",
    "\n",
    "config_opt_experiment = workflow_dir / 'configs' / 'config_optimizer_experiment.yml'\n",
    "config_opt_experiment.write_text(yaml.safe_dump(cfg))\n",
    "config_opt_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4cd5f1-925c-4f0f-9029-d329358de6b2",
   "metadata": {},
   "source": [
    "**Reminder:** Any parameter marked as optimizable but lacking a search space in both code and config leads to a runtime error, so provide overrides where needed.\n",
    "\n",
    "Version these experiment configs alongside your codebase. When you discover tighter bounds that work well (for example, a narrower `decision_threshold` window), you can promote them into the main configuration and keep the history of how you got there.\n",
    "\n",
    "## Understand the Optimizer’s Evaluation Loop\n",
    "\n",
    "The optimizer reuses the `eval` block in the config. That means:\n",
    "- Each trial runs the full phishing workflow on `smaller_test.csv`.\n",
    "- Evaluators (`rag_accuracy`, `rag_groundedness`, `token_efficiency`, `llm_latency`, `phishing_accuracy`) score the outputs.\n",
    "\n",
    "\n",
    "Later, increase `reps_per_param_set` above 1 so each trial averages multiple runs. That stabilises the metrics when evaluators involve nondeterministic judges or when the workflow itself has inherent variability.\n",
    "\n",
    "## Launch the Optimizer - This will take a while to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07231fd5-15e0-460b-a88e-37793fcb7e50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! nat optimize --config_file configs/config_optimizer.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52144280-48d3-4fce-a44f-a64eaf243ec3",
   "metadata": {},
   "source": [
    "**What to watch in the log:**\n",
    "- Optuna trial summaries showing parameter suggestions and objective scores.\n",
    "- GA generation summaries with fitness rankings and diversity scores.\n",
    "- References to evaluator outputs stored per trial.\n",
    "- Checkpoints for the best numeric trial and best prompt set so far.\n",
    "\n",
    "If the run stops unexpectedly (for instance, due to a transient rate limit), rerun the command once the issue clears. At the moment the study is in-memory, so the optimizer restarts; however, any per-trial configs already written under the output directory give you a breadcrumb trail to resume analysis manually.\n",
    "\n",
    "## Explore the Optimizer Output Directory\n",
    "\n",
    "After the run completes, list the output directory defined in `optimizer.output_path`.\n",
    "\n",
    "Expect to find (names may vary slightly):\n",
    "- `optimized_config.yml`: Ready-to-run configuration with the best overall parameters.\n",
    "- `best_params.json`: Raw parameter dictionary for the selected trial.\n",
    "- `trials_dataframe_params.csv`: Flat table of numeric trials and their scores.\n",
    "- `pareto_front_2d.png`, `pareto_parallel_coordinates.png`, `pareto_pairwise_matrix.png`: Visual summaries.\n",
    "- `ga_history_prompts.csv`, `optimized_prompts.json`, `optimized_prompts_gen*.json`: Prompt GA artifacts.\n",
    "- `prompt_population_generation_<N>.jsonl`: Optional per-generation dumps.\n",
    "\n",
    "Because checkpoints are written incrementally, you can stop a run after numeric trials finish and still inspect the resulting study, plots, and prompt generations without rerunning everything from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6226948-da43-45de-a9f9-6da7293336d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(optimizer_output_dir.glob(\"**/*\"))[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a90fd6-a9b5-479c-9e62-164e83e4ae08",
   "metadata": {},
   "source": [
    "## Inspect Top Trials in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c30dd5-e58e-45c4-9443-95c2a7af2d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trials_csv = optimizer_output_dir / \"trials_dataframe_params.csv\"\n",
    "if trials_csv.exists():\n",
    "    trials_df = pd.read_csv(trials_csv).head()\n",
    "\n",
    "trials_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba15372b-8e63-4fe1-9733-8cda54c8dd4a",
   "metadata": {},
   "source": [
    "## Dive into Prompt Optimization Results\n",
    "Use this data to see how prompts evolved generation by generation. The GA history file also includes diversity penalties and selection metadata when `ga_diversity_lambda > 0`.\n",
    "\n",
    "Prompt optimisation tends to be the longer-running phase because every mutation issues one or more LLM calls. If progress plateaus, widen the population, increase `ga_generations`, or tweak mutation rates so fresh ideas continue entering the pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb07d5f9-cd3b-460c-ac88-92ba7a4d875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_history = optimizer_output_dir / \"ga_history_prompts.csv\"\n",
    "final_prompts = optimizer_output_dir / \"optimized_prompts.json\"\n",
    "\n",
    "if ga_history.exists():\n",
    "    hist_df = pd.read_csv(ga_history)\n",
    "    hist_df[[\"generation\"]].head()\n",
    "\n",
    "if final_prompts.exists():\n",
    "    import json\n",
    "    prompts = json.loads(final_prompts.read_text())\n",
    "    for name, prompt_text in prompts.items():\n",
    "        print(f\"Prompt '{name}' preview:\\n{prompt_text[:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f7430f-f978-4c62-af6a-062bde79d332",
   "metadata": {},
   "source": [
    "## Explore Pareto Trade-Offs\n",
    "\n",
    "When optimizing multiple metrics, there may not be a single \"best\" trial. Review Pareto plots to select a configuration that matches your risk appetite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccda8e6-43c6-44ae-b5f2-85e3083fc87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_plot = optimizer_output_dir / \"plots\"/ \"pareto_parallel_coordinates.png\"\n",
    "pareto_plot.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f21e7e7-0079-46c2-afc9-47d0fb5f49f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=pareto_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000f2fb-abd8-4ca5-ada2-5428ccf7acfa",
   "metadata": {},
   "source": [
    "For deeper analysis, read the `trials_dataframe_params.csv` file into pandas and recreate Pareto filters interactively—handy when you want to annotate candidate configurations with business-specific thresholds or visualise them directly inside a notebook.\n",
    "\n",
    "## Next Experiments\n",
    "\n",
    "1. **Experiment with weights:** Adjust `eval_metrics.<metric>.weight` and `multi_objective_combination_mode` to emphasize the trade-offs you care about.\n",
    "2. **Increase robustness:** Set `reps_per_param_set` > 1 for noisier datasets. It averages scores and stabilizes rankings.\n",
    "3. **Expand datasets:** Swap `smaller_test.csv` for a larger evaluation set to tune against production-like data.\n",
    "4. **Hybrid search:** Start with numeric tuning, lock in the best LLM hyperparameters, then rerun prompt GA only.\n",
    "5. **Automate:** Add `nat optimize` to CI (nightly or weekly) and alert on improvements or regressions in best trial scores.\n",
    "\n",
    "Document the context for each optimizer run (dataset snapshot, metric weights, notable prompts) in a short `README` inside the output directory. That breadcrumb trail makes it far easier to defend decisions and revisit successful experiments later.\n",
    "\n",
    "Congratulations — you now have a full loop: *evaluate*, *profile*, and *optimize* the Email Phishing Analyzer. Keep iterating and share your best configs with the team so everyone benefits.\n",
    "\n",
    "Happy optimizing!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}