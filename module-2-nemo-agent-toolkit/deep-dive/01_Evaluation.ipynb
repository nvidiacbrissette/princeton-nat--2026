{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e35416e-3a8e-4645-b8a7-d9a036b19b78",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f7c61d-59e8-44dc-83d1-45174c7b135d",
   "metadata": {},
   "source": "> **Deep Dive**: This notebook is part of the deep-dive series that extends [03_Evaluation_Observability_And_Optimization.ipynb](../03_Evaluation_Observability_And_Optimization.ipynb). In the previous notebooks, you were introduced to evaluation, observability, and optimization using a simple math agent. This deep-dive takes those same concepts to production depth using a real-world email phishing analyzer workflow with custom evaluators, advanced profiling, and multi-objective optimization."
  },
  {
   "cell_type": "markdown",
   "id": "b04c0a77-6377-4835-9956-05e233c73d79",
   "metadata": {},
   "source": [
    "# Email Phishing Analyzer Evaluation Notebook\n",
    "\n",
    "Welcome! This notebook-style walkthrough is designed to help anyone new to the NeMo Agent Toolkit (NAT) build intuition for how workflow evaluation works. We will explore the Email Phishing Analyzer example, but the principles carry over to any NAT workflow you build."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdf0c3d-64c5-4516-8a9f-da81cafc10d3",
   "metadata": {},
   "source": [
    "## Orientation & Learning Goals\n",
    "\n",
    "**Why evaluate?**\n",
    "\n",
    "Evaluation is how you verify that agent behaviors are reliable before shipping them to users or chaining them into larger systems. With NAT you can:\n",
    "- Run the entire workflow against a labeled dataset to surface regressions early.\n",
    "- Layer multiple evaluators (LLM-based or deterministic) for richer insights than \"pass/fail\".\n",
    "- Capture telemetry for model cost, latency, or prompt usage while scoring accuracy.\n",
    "\n",
    "**In this notebook you will learn how to:**\n",
    "- Inspect the phishing workflow architecture and understand how its tools cooperate.\n",
    "- Configure NAT's evaluation stack, combining built-in evaluators with a custom metric.\n",
    "- Execute `nat eval`, read the produced artifacts, and reason about the results.\n",
    "- Iterate on prompts, parameters, and evaluators while keeping metrics trustworthy.\n",
    "\n",
    "> **Mindset Shift:** Treat evaluation configs like tests. Commit them. Run them on every change. That way a \"phishing detector\" stays trustworthy even as you tweak prompts or swap models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fd3832-e609-4891-b104-a4b10e509570",
   "metadata": {},
   "source": [
    "## System Prerequisites & Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbeb8a4-c93d-4a9b-adca-1e59461a9e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the phishing workflow package in editable mode\n",
    "# ! uv pip install -e .\n",
    "# ! uv pip install -U langchain\n",
    "# Confirm the CLI entry point is available\n",
    "! nat --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b800db-42f7-4180-a97a-8f10c3d694d0",
   "metadata": {},
   "source": [
    "**Before moving on:**\n",
    "For this notebook, you will need the following API keys to run all examples end-to-end:\n",
    "\n",
    "NVIDIA Build: You can obtain an NVIDIA Build API Key by creating an NVIDIA Build account and generating a key at https://build.nvidia.com/settings/api-keys\n",
    "Then you can run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759f06e5-1c34-428e-a529-6b8925abf5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"NVIDIA_API_KEY\" not in os.environ:\n",
    "    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11ce2b-6a87-4c7c-9830-046d82655951",
   "metadata": {},
   "source": [
    "## Anchor Key Paths for the Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167fb702-f1e8-4ca2-89cb-d21f8541d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root = Path.cwd()\n",
    "workflow_dir = root\n",
    "config_path = workflow_dir / \"configs\" / \"config.yml\"\n",
    "data_path = workflow_dir / \"data\" / \"smaller_test.csv\"\n",
    "config_path, data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6d61da-f6f6-4fd6-bb09-22b7a92342cf",
   "metadata": {},
   "source": [
    "**Why this matters:** Treating these paths as variables avoids chasing typos later. It also reminds you where the evaluation artifacts will land (`config_path` controls the output directory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47a881-3620-4dec-99e0-d2565ad046bd",
   "metadata": {},
   "source": [
    "## Understand the Workflow Components\n",
    "\n",
    "The phishing analyzer is built as a `react_agent` that coordinates several tools:\n",
    "\n",
    "- `sensitive_info_detector`: asks an LLM whether the email requests sensitive data.\n",
    "- `intent_classifier`: classifies likely attacker intent (credential theft, fraud, malware, etc.).\n",
    "- `link_and_domain_analyzer`: a deterministic helper that spots suspicious URLs locally.\n",
    "- `phishing_risk_aggregator`: aggregates the tool outputs into a structured verdict.\n",
    "- `email_phishing_analyzer`: a convenience tool for single-step direct calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc060f-bbcb-43cf-92e9-7585042d05a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "config = yaml.safe_load(config_path.read_text())\n",
    "config[\"workflow\"], list(config[\"functions\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d9a67-1fe1-4b7d-af3e-ced6f1939989",
   "metadata": {},
   "source": [
    "**Key takeaway:** Evaluation is only meaningful if you understand the workflow outputs. Here, the expected output is JSON with fields such as `is_likely_phishing`, `risk_score`, and `factors`. Keep that schema in mind when interpreting evaluator results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2198e53e-5329-4a55-b05f-d7672c6c117b",
   "metadata": {},
   "source": [
    "## Mental Model for NAT Evaluation\n",
    "\n",
    "NAT's evaluation pipeline is opinionated but flexible. Think of it as three building blocks that you plug together in `config.yml`:\n",
    "\n",
    "1. **General settings (`eval.general`)** — Where to write outputs, which dataset to load, whether to stream verbose logs, and optional profiler/telemetry toggles.\n",
    "2. **Evaluators (`eval.evaluators`)** — A map of evaluator names to evaluator configs (built-in or custom) that will score every workflow run. Multiple evaluators can score the same run.\n",
    "3. **Shared resources** — Datasets, tool registry entries, and judge LLMs referenced by the evaluators. These typically live in other sections of the same config file (`dataset`, `llms`, `functions`).\n",
    "\n",
    "```yaml\n",
    "# Excerpt: eval.general block from config.yml\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: ./.tmp/eval/examples/evaluation_and_profiling/email_phishing_analyzer/original\n",
    "    verbose: true\n",
    "    dataset:\n",
    "      _type: csv\n",
    "      file_path: examples/evaluation_and_profiling/email_phishing_analyzer/data/smaller_test.csv\n",
    "      id_key: \"subject\"\n",
    "      structure:\n",
    "        question_key: body\n",
    "        answer_key: label\n",
    "```\n",
    "\n",
    "**Design notes:**\n",
    "- `id_key` provides a stable identifier so every evaluator output can be joined back to the dataset row.\n",
    "- `question_key`/`answer_key` tell NAT how to feed the workflow (`body`) and where to find ground truth (`label`).\n",
    "- The nested `profiler` dictionary (scroll further down in `config.yml`) allows runtime forecasts, token accounting, and concurrency analysis to run alongside evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65107eb-46fc-45a7-ae84-9299a13c6add",
   "metadata": {},
   "source": [
    "## Meet the Built-in Evaluators\n",
    "\n",
    "The workflow reuses several evaluators that ship with NAT:\n",
    "\n",
    "- **RAGAS-backed metrics** (`rag_accuracy`, `rag_groundedness`, `rag_relevance`): Judge the generated answer against the ground truth and context. They rely on a judge LLM defined in `llms` (`nim_rag_eval_llm`).\n",
    "- **Trajectory evaluator** (`trajectory_accuracy`): Asks a judge LLM to grade the entire agent tool-call sequence.\n",
    "\n",
    "```yaml\n",
    "# Excerpt: built-in evaluators in config.yml\n",
    "  evaluators:\n",
    "    rag_accuracy:\n",
    "      _type: ragas\n",
    "      metric: AnswerAccuracy\n",
    "      llm_name: nim_rag_eval_llm\n",
    "    rag_groundedness:\n",
    "      _type: ragas\n",
    "      metric: ResponseGroundedness\n",
    "      llm_name: nim_rag_eval_llm\n",
    "    rag_relevance:\n",
    "      _type: ragas\n",
    "      metric: ContextRelevance\n",
    "      llm_name: nim_rag_eval_llm\n",
    "    trajectory_accuracy:\n",
    "      _type: trajectory\n",
    "      llm_name: nim_trajectory_eval_llm\n",
    "```\n",
    "\n",
    "**How to reason about the scores:**\n",
    "- All of these metrics return floats in `[0, 1]`. Higher is better.\n",
    "- If you receive a low `ResponseGroundedness`, inspect the retrieved evidence — the workflow might be hallucinating explanations.\n",
    "- Low `Trajectory` scores usually mean the agent took suboptimal tool actions even if the final answer looks correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4945c64-422b-4f88-932a-15a961dcdf93",
   "metadata": {},
   "source": [
    "## Deep Dive on the Custom Phishing Evaluator\n",
    "\n",
    "Builtin metrics are great, but this workflow also ships a purpose-built evaluator: `phishing_accuracy`. It translates the workflow’s JSON verdict into classic binary accuracy against the dataset labels.\n",
    "\n",
    "```python\n",
    "# In [6]: excerpt from src/nat_email_phishing_analyzer/evaluator_register.py\n",
    "from nat.eval.evaluator.base_evaluator import BaseEvaluator\n",
    "from nat.eval.evaluator.evaluator_model import EvalOutputItem\n",
    "\n",
    "class PhishingAccuracyEvaluatorConfig(EvaluatorBaseConfig, name=\"phishing_accuracy\"):\n",
    "    metric_name: str = \"accuracy\"\n",
    "\n",
    "@register_evaluator(config_type=PhishingAccuracyEvaluatorConfig)\n",
    "async def register_phishing_accuracy_evaluator(config, _builder):\n",
    "    class PhishingAccuracy(BaseEvaluator):\n",
    "        async def evaluate_item(self, item):\n",
    "            label = str(item.full_dataset_entry.get(\"label\", \"\")).strip().lower()\n",
    "            expected_is_phish = label in {\"phish\", \"phishing\", \"spam\", \"malicious\"}\n",
    "\n",
    "            output = item.output_obj\n",
    "            is_phish_pred = False\n",
    "            try:\n",
    "                import json\n",
    "                parsed = json.loads(output) if isinstance(output, str) else output\n",
    "                if isinstance(parsed, dict):\n",
    "                    is_phish_pred = bool(parsed.get(\"is_likely_phishing\", False))\n",
    "                else:\n",
    "                    is_phish_pred = isinstance(output, str) and \"likely a phishing\" in output.lower()\n",
    "            except Exception:\n",
    "                is_phish_pred = isinstance(output, str) and \"likely a phishing\" in output.lower()\n",
    "\n",
    "            score = 1.0 if (is_phish_pred == expected_is_phish) else 0.0\n",
    "            reasoning = {\"expected_label\": label, \"predicted_is_phish\": is_phish_pred}\n",
    "            return EvalOutputItem(id=item.id, score=score, reasoning=reasoning)\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "- Evaluators receive both the workflow output (`item.output_obj`) and the full dataset row (`item.full_dataset_entry`). You can use extra columns for richer debugging.\n",
    "- It is safe to implement fallback heuristics (like substring matches) if you expect occasional non-JSON outputs.\n",
    "- Registering an evaluator is as simple as decorating it with `@register_evaluator` and yielding an `EvaluatorInfo`.\n",
    "  \n",
    "## Explore the Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9c1fde-a429-4238-8eb3-53a55b964239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "preview = pd.read_csv(data_path)\n",
    "preview[[\"subject\", \"body\", \"label\"]].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da73a47-78a5-4b4b-843e-91f143bc33ec",
   "metadata": {},
   "source": [
    "**Dataset schema refresher:**\n",
    "- `subject` (string): used as the evaluation item ID.\n",
    "- `body` (string): the email body fed into the agent.\n",
    "- `label` (string): ground truth (`phish` or `benign`).\n",
    "- Additional columns (`intents`, `source`, etc.) travel with the evaluation item and can inform debugging or future metrics.\n",
    "\n",
    "> **Quality tip:** Start with a small dataset like `smaller_test.csv` to smoke-test evaluators. As the workflow stabilizes, grow the dataset and keep it version-controlled so improvements are measurable.\n",
    "\n",
    "Before moving on, take a minute to inspect class balance and metadata coverage (for example, `preview['label'].value_counts()` or `preview['sender'].nunique()`). Knowing whether your dataset leans heavily toward \"phish\" or \"benign\" helps you pick metrics that matter (recall vs precision), and spotting sparse columns early prevents confusion when you later rely on them in evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a9e46b-512b-41ce-addb-9e61b02f8930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7cb45-996d-4258-bc8d-54ec46c66d91",
   "metadata": {},
   "source": [
    "## Sanity-Check the Evaluation Plan\n",
    "\n",
    "Before hitting \"run\", double-check the pieces we rely on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a10056a-8b85-4a61-8013-455462a774b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert config_path.exists(), \"Missing config file\"\n",
    "assert data_path.exists(), \"Missing evaluation dataset\"\n",
    "assert \"phishing_accuracy\" in config[\"eval\"][\"evaluators\"], \"Custom evaluator not wired\"\n",
    "config[\"llms\"].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e75288e-7401-48db-bc04-43a839de905f",
   "metadata": {},
   "source": [
    "**Why pause here?**\n",
    "- Failing fast on missing assets saves time, especially when collaborating with teammates.\n",
    "- Listing the available LLMs reminds you which judge models will be billed during evaluation.\n",
    "\n",
    "Consider setting `eval.general.output_dir` to a timestamped folder during experimentation so you keep historical runs side-by-side for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a382aa-7001-465a-a637-7a9bc92fa29d",
   "metadata": {},
   "source": [
    "## Run the Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f56c5ad-eca9-47c5-b905-18d451fe74c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! nat eval --config_file configs/config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58500b54-0d12-40b8-a330-82e59a3f90a2",
   "metadata": {},
   "source": [
    "**What you should see:**\n",
    "- NAT prints the workflow summary, then streams progress as each dataset row is processed.\n",
    "- For each evaluator you'll get an aggregate score at the end. With `verbose: true`, per-item logs appear too.\n",
    "- Outputs are written under `eval.general.output_dir`. If the directory doesn't exist NAT creates it.\n",
    "\n",
    "> **Troubleshooting:** If you encounter `[429] Too Many Requests`, reduce judge LLM concurrency using `eval.general.max_concurrency`. For network hiccups, rerunning the command resumes where it left off as long as the output directory is intact.\n",
    "\n",
    "Need to change datasets or tweak runtime behaviour without editing YAML? Run `nat eval --help` to discover overrides such as `--dataset`, `--endpoint`, `--reps`, or `--override`. These flags make it easy to spin up quick experiments while keeping the committed config untouched."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd87cca-d452-4679-bd2e-bad523f61353",
   "metadata": {},
   "source": [
    "## Inspect Evaluation Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d963f0-8736-4b7b-ac9e-377524ab0fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "output_dir = Path(config[\"eval\"][\"general\"][\"output_dir\"])\n",
    "phishing_report = json.loads((output_dir / \"phishing_accuracy_output.json\").read_text())\n",
    "rag_accuracy_report = json.loads((output_dir / \"rag_accuracy_output.json\").read_text())\n",
    "\n",
    "print(f\"The average phishing report score was {phishing_report[\"average_score\"]} and the average RAGAS accuracy was {rag_accuracy_report[\"average_score\"]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13ff7d-edaf-49ad-8a3e-e5468c595a4a",
   "metadata": {},
   "source": [
    "Dive deeper into individual examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7ab762-c821-4824-8265-9843720aee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item = phishing_report[\"eval_output_items\"][0]\n",
    "first_item[\"id\"], first_item[\"score\"], first_item[\"reasoning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f48b06-9fe0-4586-9854-f304e21c7e0d",
   "metadata": {},
   "source": [
    "**Artifacts generated:**\n",
    "- `workflow_output.json`: every workflow run plus intermediate steps — perfect for understanding *why* an evaluator scored an item poorly.\n",
    "- `<evaluator_name>_output.json`: evaluator-specific metrics. Many built-in evaluators include `judgment` or `explanation` fields; custom ones can add any debugging payload you need.\n",
    "- Optional profiler outputs: CSV/JSON snapshots that estimate runtime, latency bottlenecks, or token uniqueness if those toggles are enabled in the config.\n",
    "\n",
    "Each evaluator JSON shares a consistent structure (`average_score`, `eval_output_items`, optional `metadata`), which means you can parse them with a single helper and aggregate across runs. Consider enriching `reasoning` with extra context (for example, GUIDs or remediation links) so triage engineers can jump straight to action items when a score dips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2345fa20-71e9-4f3e-ac83-606755397ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore more evaluation output here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f92079-4922-408c-890b-057a9703c77e",
   "metadata": {},
   "source": [
    "## Wrap-Up & Next Steps\n",
    "\n",
    "Congratulations! You now understand how to:\n",
    "- Wire up datasets, built-in evaluators, and custom metrics in NAT.\n",
    "- Execute `nat eval` and interpret the results with confidence.\n",
    "- Iterate thoughtfully, keeping evaluation at the center of your workflow development cycle.\n",
    "\n",
    "**Where to go from here:**\n",
    "1. Expand the dataset with real phishing/benign emails your team cares about.\n",
    "2. Add evaluators that capture business-specific risks (false negatives may cost more than false positives).\n",
    "3. Automate: run this evaluation in CI or scheduled jobs so you always know when performance drifts.\n",
    "\n",
    "Happy evaluating! Share your discoveries with the team so everyone benefits from the improvements you make."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}