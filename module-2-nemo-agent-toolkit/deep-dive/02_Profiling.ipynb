{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "nvidia-branding-header",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deep-dive-framing",
   "metadata": {},
   "source": "> **Deep Dive**: This notebook is part of the deep-dive series that extends [03_Evaluation_Observability_And_Optimization.ipynb](../03_Evaluation_Observability_And_Optimization.ipynb). In the previous notebooks, you were introduced to evaluation, observability, and optimization using a simple math agent. This deep-dive takes those same concepts to production depth using a real-world email phishing analyzer workflow with custom evaluators, advanced profiling, and multi-objective optimization."
  },
  {
   "cell_type": "markdown",
   "id": "135811a5-73e9-4cca-b5d0-3449a86c2ea0",
   "metadata": {},
   "source": [
    "# Email Phishing Analyzer Profiling Notebook\n",
    "\n",
    "Welcome to the companion \"notebook\" for profiling the Email Phishing Analyzer workflow. If you just finished the evaluation notebook, this guide picks up where accuracy left off and shows you how to reason about runtime, cost, and scaling characteristics using the NeMo Agent Toolkit (NAT) Profiler.\n",
    "\n",
    "\n",
    "## Why Profile?\n",
    "\n",
    "**Profiling goals for phishing detection**\n",
    "- Reveal which tools and LLM calls dominate latency so you can trim response times before production.\n",
    "- Forecast future workload characteristics (tokens, runtime) using the profiler's forecasting models.\n",
    "- Detect concurrency spikes or bottlenecks that could overwhelm shared services.\n",
    "- Surface prompt fragments worth caching to reduce repeated work and latency.\n",
    "\n",
    "> **Mental model:** Evaluation tells you \"*Did we get the right answer?*\" Profiling adds \"*How expensive was that answer, and what happens under load?*\" Keep both in sync for a resilient workflow.\n",
    "\n",
    "## Prerequisites & Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f606fd2-db42-4e3c-9dce-0999ac6b0956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install the phishing workflow package in editable mode\n",
    "! uv pip install -e .\n",
    "\n",
    "# Confirm the CLI entry point is available\n",
    "! nat --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00b728-6119-4476-82e6-57c9455a1c27",
   "metadata": {},
   "source": [
    "**Before moving on:**\n",
    "For this notebook, you will need the following API keys to run all examples end-to-end:\n",
    "\n",
    "NVIDIA Build: You can obtain an NVIDIA Build API Key by creating an NVIDIA Build account and generating a key at https://build.nvidia.com/settings/api-keys\n",
    "Then you can run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353ec54a-abbc-4e8a-b478-4361b38dc61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"NVIDIA_API_KEY\" not in os.environ:\n",
    "    nvidia_api_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvidia_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e956df7-fc94-408a-9707-1604c429a714",
   "metadata": {},
   "source": [
    "## Establish Paths & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e91505a-0862-4d3d-9027-4f8cd48d04f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "root = Path.cwd()\n",
    "workflow_dir = root\n",
    "config_path = workflow_dir / \"configs\" / \"config_profiler.yml\"\n",
    "profile_output_dir = Path(\"eval_output_with_profiler\")\n",
    "config_path, profile_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef656e85-02aa-41c8-bfba-eab996cb4ced",
   "metadata": {},
   "source": [
    "**Tip:** Switch `profile_output_dir` to a timestamped folder (e.g., `profile_output_dir = root / '.tmp' / f\"phishing_profile_{datetime.utcnow():%Y%m%d_%H%M%S}\"`) while iterating so each profiling run has its own snapshot.\n",
    "\n",
    "## Understand Instrumentation Hooks\n",
    "\n",
    "Profiling works best when the workflow is instrumented. The phishing analyzer already opts into profiling decorators via `framework_wrappers` when registering each tool.\n",
    "\n",
    "\n",
    "```python\n",
    "@register_function(\n",
    "    config_type=EmailPhishingAnalyzerConfig,\n",
    "    framework_wrappers=[LLMFrameworkEnum.LANGCHAIN],\n",
    ")\n",
    "async def email_phishing_analyzer(\n",
    "    config: EmailPhishingAnalyzerConfig, builder: Builder\n",
    ") -> Any:\n",
    "    \"\"\"Register the email phishing analysis tool.\"\"\"\n",
    "```\n",
    "\n",
    "**What to look for:** A value like `[LLMFrameworkEnum.LANGCHAIN]` confirms NAT will wrap LangChain calls with profiler callbacks. If you extend the workflow with new frameworks (e.g., LlamaIndex or CrewAI), add the appropriate enum to keep instrumentation intact.\n",
    "\n",
    "> **If you build new tools:** add `framework_wrappers=[LLMFrameworkEnum.LANGCHAIN]` (or the relevant framework enum) in your `@register_function` decorator. That ensures every LLM call emits usage stats into the profiler trace.\n",
    "\n",
    "## Inspect Profiler Configuration in `config.yml`\n",
    "\n",
    "The profiler ships as part of the evaluation configuration. Scroll to the `eval.general.profiler` block to see the toggles enabled for this workflow.\n",
    "\n",
    "```yaml\n",
    "# Excerpt from config.yml\n",
    "  general:\n",
    "    output_dir: eval_outout_with_profiler\n",
    "    verbose: true\n",
    "    dataset:\n",
    "      _type: csv\n",
    "      file_path: data/smaller_test.csv\n",
    "      id_key: \"subject\"\n",
    "      structure:\n",
    "        question_key: body\n",
    "        answer_key: label\n",
    "\n",
    "    profiler:\n",
    "      token_uniqueness_forecast: true\n",
    "      workflow_runtime_forecast: true\n",
    "      compute_llm_metrics: true\n",
    "      csv_exclude_io_text: true\n",
    "      prompt_caching_prefixes:\n",
    "        enable: true\n",
    "        min_frequency: 0.1\n",
    "      bottleneck_analysis:\n",
    "        enable_nested_stack: true\n",
    "      concurrency_spike_analysis:\n",
    "        enable: true\n",
    "        spike_threshold: 7\n",
    "```\n",
    "\n",
    "**Interpretation guide:**\n",
    "- `token_uniqueness_forecast`: trains a lightweight model to predict how many *new* tokens subsequent emails will use, helping capacity planning for caching or rate limits.\n",
    "- `workflow_runtime_forecast`: forecasts runtime to anticipate SLA breaches as inputs scale.\n",
    "- `compute_llm_metrics`: produces latency/throughput summaries, percentile breakdowns, and per-tool cost stats.\n",
    "- `csv_exclude_io_text`: keeps the CSV manageable by omitting raw prompt/completion text (flip to `false` if you need them for prompt analysis).\n",
    "- `prompt_caching_prefixes`: runs PrefixSpan over prompts to suggest KV-cacheable prefixes when they appear in ≥10% of calls.\n",
    "- `bottleneck_analysis.enable_nested_stack`: surfaces nested stacks showing where time is spent (tool inside agent, etc.). Swap to `simple_stack` for a flatter summary.\n",
    "- `concurrency_spike_analysis`: alerts when concurrent calls exceed the specified threshold (7 here). Adjust based on your throughput targets.\n",
    "\n",
    "Treat these toggles like building blocks: start with `compute_llm_metrics` for a quick latency snapshot, then layer forecasting or prefix mining once you trust the basics. Each option adds some processing time, so tailoring them to the question at hand keeps profiling runs snappy.\n",
    "\n",
    "## Optional: Customize Profiler Settings on the Fly\n",
    "\n",
    "You can clone the base config and tweak profiler parameters without editing the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e11a65-af91-42ef-88a7-8d6a6fc4b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "profile_config = yaml.safe_load(config_path.read_text())\n",
    "profile_config[\"eval\"][\"general\"][\"output_dir\"] = \"./.tmp/eval/email_phishing_analyzer/profile_run_01\"\n",
    "profile_config[\"eval\"][\"general\"][\"profiler\"][\"concurrency_spike_analysis\"][\"spike_threshold\"] = 5\n",
    "profile_config_path = workflow_dir / \"configs\" / \"config_profile_experiment.yml\"\n",
    "profile_config_path.write_text(yaml.safe_dump(profile_config))\n",
    "profile_config_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f3248f-de8f-4402-86e9-9cf4e09534ad",
   "metadata": {},
   "source": [
    "**Good practice:** Treat profiling configs like experiments. Check them into version control with clear names so you can compare runs later (`profile_run_{date}_{change}`).\n",
    "\n",
    "While cloning configs, adjust logging thresholds or telemetry sinks as well. Profiling often uncovers long-tail events, and richer logs will help you connect a latency spike back to the original agent thoughts or tool inputs.\n",
    "\n",
    "## Run the Workflow with Profiling Enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebd80f-a7df-4995-8674-496879c8273b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! nat eval --config_file configs/config_profiler.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1209f-92ce-48fe-a344-b3d55d339f8b",
   "metadata": {},
   "source": [
    "Expect the CLI to log both evaluation scores and profiler status updates. With `verbose: true`, you will see when forecasting models train and when reports are written.\n",
    "\n",
    "> **Time-saving tip:** When iterating quickly, limit the dataset via `dataset.limit` (see the evaluation doc) so profiling runs faster while you dial in settings.\n",
    "\n",
    "If you ever need to run profiling against a remote workflow (`nat eval --endpoint ...`), the profiler works the same way: evaluation still happens client-side, so make sure your local machine has access to the generated artifacts.\n",
    "\n",
    "## Navigate the Profiler Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a293ec8-351f-4366-b589-47fd450a3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(profile_output_dir.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c660824f-c435-4091-914e-bc3b189998d3",
   "metadata": {},
   "source": [
    "You should see files similar to:\n",
    "- `all_requests_profiler_traces.json`\n",
    "- `standardized_data_all.csv`\n",
    "- `inference_optimization.json`\n",
    "- Optional reports (e.g., `bottleneck_analysis_report.json`, `concurrency_spikes.json`, `prompt_prefixes.json`) depending on enabled toggles.\n",
    "\n",
    "If the directory is empty, double-check you ran the evaluation command against the correct config and that the workflow produced at least one successful run.\n",
    "\n",
    "A quick sanity check is to open `profile_output_dir / \"standardized_data_all.csv\"` and ensure there are rows for each tool you expected to fire. Zero rows often means the workflow short-circuited before hitting the instrumented functions.\n",
    "\n",
    "## Explore Raw Traces\n",
    "\n",
    "`all_requests_profiler_traces.json` captures every instrumented call (LLM/tool) with timestamps, token counts, and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b3249-97f6-4360-a435-1e34ed42cb59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "trace_path = profile_output_dir / \"all_requests_profiler_traces.json\"\n",
    "traces = json.loads(trace_path.read_text())\n",
    "print(json.dumps(traces[0], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edf57ee-5b32-4c64-85e2-b488591c6910",
   "metadata": {},
   "source": [
    "**Use cases:**\n",
    "- Validate that each tool call is captured (look for `sensitive_info_detector`, `intent_classifier`, etc.).\n",
    "- Investigate spikes by filtering on `metrics.latency_ms` or `metrics.tokens`.\n",
    "- Feed these traces into custom dashboards if you need deeper visualization.\n",
    "\n",
    "Each trace event also includes `inputs` and `outputs` metadata (sanitised if `csv_exclude_io_text` is true). Use that to correlate long-running calls with specific prompts or payload sizes—an invaluable clue when you suspect oversized inputs are slowing things down.\n",
    "\n",
    "## Analyze Standardized CSV Data\n",
    "\n",
    "`standardized_data_all.csv` is a tabular view ideal for quick pandas analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adbfd76-bf2d-4aa9-bd05-2340eb86d310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "csv_path = profile_output_dir / \"standardized_data_all.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Inspect columns relevant to latency & tokens\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99979e-832b-4151-92ec-192ef75d9fa2",
   "metadata": {},
   "source": [
    "**Insights to extract:**\n",
    "- High average latency in `phishing_risk_aggregator` may suggest prompt optimization or a smaller LLM.\n",
    "- Large `prompt_tokens` for `sensitive_info_detector` hints at multi-turn agent traces that you might truncate or compress.\n",
    "- Compare `latency_ms` percentiles (90th vs 99th) using `df.latency_ms.quantile([0.9, 0.99])` to size buffers for worst-case scenarios.\n",
    "\n",
    "Because the CSV is standardized, you can combine multiple runs into a single DataFrame with a `run_id` column and build lightweight dashboards (box plots per component, token histograms, etc.) directly in pandas or your BI tool of choice.\n",
    "\n",
    "\n",
    "## Optimization Reports\n",
    "\n",
    "The profiler synthesizes higher-level insights in JSON summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cdba80-b1ee-49e9-9cf8-94c0c188c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_path = profile_output_dir / \"inference_optimization.json\"\n",
    "with opt_path.open() as f:\n",
    "    optimization_report = json.load(f)\n",
    "optimization_report.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ffd4ce-8b5c-43ef-81e6-60325f845572",
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_stats = optimization_report[\"workflow_runtimes\"]\n",
    "print(f\"The p90 workflow runtime was {runtime_stats['p90']} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e66a23-43c5-4070-bc29-737f9c336803",
   "metadata": {},
   "source": [
    "The optimization report also records confidence intervals and percentiles for LLM latency and agent request throughput. Run the cell below to explore those values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923638c8-e5f4-4312-b4a2-04e54aea4ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimization_report['confidence_intervals']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c8ba4-8bc2-4e2f-b5d6-d7497352a8a4",
   "metadata": {},
   "source": [
    "## Concurrency Insights\n",
    "\n",
    "When `bottleneck_analysis.enable_nested_stack` is on, the profiler also runs the workflow at varying levels of concurrency to understand the latency and concurrency profile of your workflow. For example, run the cell below to load the results of the stack and concurrency analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebf057-b6a9-451b-9647-2a1e02a5cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "opt_path = profile_output_dir / \"workflow_profiling_metrics.json\"\n",
    "with opt_path.open() as f:\n",
    "    analysis_report = json.load(f)\n",
    "\n",
    "analysis_report.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ecd7a-3858-4182-baf7-127495ed0fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"The average LLM latency at various tested concurrencies of the workflow was \n",
    "{analysis_report['concurrency_spike_analysis']['average_latency_by_concurrency']}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e89d2d-ecbc-40e8-bae3-82cff0469efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore other parts of the analysis report here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb93acd4-8dd4-44d9-ba64-659c87f28184",
   "metadata": {},
   "source": [
    "## Viewing Your Agent Execution\n",
    "\n",
    "The profiler also produces a Gantt chart that you can use to visualize the execution of your agent to visually spot bottlenecks or understand performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321ab641-69bb-4acb-acd6-b546ec02ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "img_path = profile_output_dir / \"gantt_chart.png\"\n",
    "Image(filename=img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f612c9fe-e6dd-435d-8191-49245acd73eb",
   "metadata": {},
   "source": [
    "## Take Action on Profiling Insights\n",
    "\n",
    "- **Reduce latency:** If a single tool dominates runtime, experiment with a smaller LLM (`llama_3_70`), tighten prompts, or reduce unnecessary intermediate calls.\n",
    "- **Manage concurrency:** Add `max_concurrency` to your config or throttle tool usage when spikes exceed infrastructure limits.\n",
    "- **Cache smartly:** Use `prompt_prefixes.json` output to configure KV caching. Combine with `token_uniqueness_forecast` to prioritize caches where new tokens are rare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}