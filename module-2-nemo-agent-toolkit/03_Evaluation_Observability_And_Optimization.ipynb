{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.nvidia.com/dli\"> <img src=\"images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\"> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating, Observing, and Optimizing NeMo Agent Toolkit Workflows\n",
    "\n",
    "You will walk through how to set up agent evaluation, observability, and optimization in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating, Observing, and Optimizing Our Agent\n",
    "\n",
    "A key component of the NeMo Agent Toolkit is that it can run several well-known evaluations against agentic workflows. Proper evaluation helps us:\n",
    "\n",
    "### 1. Why Evaluate?\n",
    "- **Measure Performance**: Quantify how well our agent performs on specific tasks\n",
    "- **Identify Weaknesses**: Find edge cases or failure modes\n",
    "- **Compare Versions**: Track improvements across different iterations\n",
    "- **Ensure Reliability**: Verify the agent works consistently\n",
    "\n",
    "### 2. Evaluation Process\n",
    "1. **Create Test Data**: Define questions with known answers\n",
    "2. **Configure Evaluators**: Set up metrics to measure performance\n",
    "3. **Run Evaluation**: Process all test cases\n",
    "4. **Analyze Results**: Review metrics and identify areas for improvement\n",
    "\n",
    "### 3. Available Metrics\n",
    "- **Answer Accuracy**: How correct are the agent's responses?\n",
    "- **Context Relevance**: Is the agent using appropriate context?\n",
    "- **Response Groundedness**: Are responses based on retrieved information?\n",
    "- **Trajectory Analysis**: Is the agent's reasoning process sound?\n",
    "\n",
    "This notebook will show how to run evaluations and observe our agent as it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's load our environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom dotenv import find_dotenv, load_dotenv\nload_dotenv(find_dotenv())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Methods in the NeMo Agent Toolkit\n",
    "\n",
    "The NeMo Agent Toolkit provides several built-in evaluators to assess the performance of your workflows:\n",
    "\n",
    "1. **RAGAS Evaluator**: An open-source evaluation framework for RAG (Retrieval-Augmented Generation) workflows. RAGAS provides metrics like Answer Accuracy, Context Relevance, and Response Groundedness.\n",
    "\n",
    "2. **Trajectory Evaluator**: Uses the intermediate steps generated by the workflow to evaluate the agent's reasoning process and decision-making path.\n",
    "\n",
    "3. **SWE-Bench Evaluator**: Specifically designed for software engineering tasks, this evaluator tests if the agent can solve programming problems by running tests on the generated code.\n",
    "\n",
    "In this notebook, we'll primarily use the **RAGAS Evaluator** and **Trajectory Evaluator** to assess our math tools agent.\n",
    "\n",
    "Let's create a directory to store evaluation data. This will contain test cases with questions and expected answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p workflows/math_tools/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next cell, we will create an evaluation JSON file.\n",
    "\n",
    "It will include both standard and time-aware test cases.\n",
    "\n",
    "Note that each test case includes the following:\n",
    "- id: A unique identifier\n",
    "- question: The input to send to our agent\n",
    "- answer: The expected correct response (or \"dynamic\" for time-based answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/math_tools/data/comprehensive_eval.json\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"question\": \"What is the square root of 49?\",\n",
    "        \"answer\": \"7\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"question\": \"Add 10 to 25\",\n",
    "        \"answer\": \"35\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"question\": \"What is the modulus of 100 divided by 3?\",\n",
    "        \"answer\": \"1\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"question\": \"What is five to the power of three?\",\n",
    "        \"answer\": \"125\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"question\": \"Is the current hour even?\",\n",
    "        \"answer\": \"dynamic\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Comprehensive Evaluation Configuration\n",
    "\n",
    "We will write a single evaluation configuration file that includes all the tools our agent needs to handle both mathematical operations and time-based queries. By now you have seen most this configuration format. This configuration includes:\n",
    "\n",
    "1. **General settings**: Where to store results and which dataset to use\n",
    "2. **Functions**: All the tools our agent will use (math operations and time functions)\n",
    "3. **LLMs**: Both the agent LLM and a separate evaluation LLM\n",
    "4. **Evaluators**: The specific metrics we want to measure\n",
    "\n",
    "The `eval` section is new. In the `eval` section, you can specify however many evaluators you want to run, calling either built-in evaluators or your own custom evaluation components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/math_tools/configs/comprehensive_eval_config.yml\n",
    "\n",
    "general:\n",
    "  use_uvloop: true\n",
    "\n",
    "functions:\n",
    "  calculator_exponent:\n",
    "    _type: calculator_exponent\n",
    "  calculator_modulus:\n",
    "    _type: calculator_modulus\n",
    "  calculator_square_root:\n",
    "    _type: calculator_square_root\n",
    "  calculator_add:\n",
    "    _type: calculator_add\n",
    "  current_datetime:\n",
    "    _type: current_datetime\n",
    "\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    temperature: 0.0\n",
    "  eval_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-405b-instruct\n",
    "    temperature: 0.0\n",
    "    max_tokens: 1024\n",
    "\n",
    "workflow:\n",
    "  _type: react_agent\n",
    "  tool_names:\n",
    "    - calculator_exponent\n",
    "    - calculator_modulus\n",
    "    - calculator_square_root\n",
    "    - calculator_add\n",
    "    - current_datetime\n",
    "  llm_name: nim_llm\n",
    "  verbose: true\n",
    "\n",
    "eval:\n",
    "  general:\n",
    "    output_dir: ./math_tools_eval/\n",
    "    dataset:\n",
    "      _type: json\n",
    "      file_path: workflows/math_tools/data/comprehensive_eval.json\n",
    "  evaluators:\n",
    "    math_accuracy:\n",
    "      _type: ragas\n",
    "      metric: AnswerAccuracy\n",
    "      llm_name: eval_llm\n",
    "    math_trajectory_accuracy:\n",
    "      _type: trajectory\n",
    "      llm_name: eval_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Comprehensive Evaluation\n",
    "\n",
    "Now let's run the evaluation using our consolidated configuration. This will:\n",
    "1. Load our test cases from the JSON file\n",
    "2. Run each test case through our agent\n",
    "3. Evaluate the responses using the specified metrics\n",
    "4. Store the results in the output directory\n",
    "\n",
    "This single evaluation run will test both standard mathematical operations and time-based queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nat eval --config_file=workflows/math_tools/configs/comprehensive_eval_config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Evaluation Results\n",
    "\n",
    "After running the evaluation, the NeMo Agent Toolkit stores the results in JSON files in our specified `output_dir`. Let's analyze these JSON results with some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Simple function to load JSON files\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Simple summary for accuracy evaluations\n",
    "def get_accuracy_summary(data):\n",
    "    items = data['eval_output_items']\n",
    "    summary = []\n",
    "    for item in items:\n",
    "        summary.append({\n",
    "            'Question': item['reasoning']['user_input'],\n",
    "            'Score': item['score']\n",
    "        })\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "# Simple summary for workflow\n",
    "def get_workflow_summary(data):\n",
    "    summary = []\n",
    "    for item in data:\n",
    "        tools = []\n",
    "        total_tokens = 0\n",
    "        steps = len(item['intermediate_steps'])\n",
    "        \n",
    "        # Count tools and tokens\n",
    "        for step in item['intermediate_steps']:\n",
    "            if 'payload' in step and 'name' in step['payload']:\n",
    "                tools.append(step['payload']['name'])\n",
    "            if 'payload' in step and 'usage_info' in step['payload']:\n",
    "                total_tokens += step['payload']['usage_info']['token_usage']['total_tokens']\n",
    "        \n",
    "        summary.append({\n",
    "            'Question': item['question'],\n",
    "            'Steps': steps,\n",
    "            'Tokens': total_tokens,\n",
    "            'Tools': ', '.join(set(tools))  # Unique tools only\n",
    "        })\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "# Load the three files\n",
    "accuracy = load_json('./math_tools_eval/math_accuracy_output.json')\n",
    "trajectory = load_json('./math_tools_eval/math_trajectory_accuracy_output.json')\n",
    "workflow = load_json('./math_tools_eval/workflow_output.json')\n",
    "\n",
    "# Create simple DataFrames\n",
    "accuracy_df = get_accuracy_summary(accuracy)\n",
    "workflow_df = get_workflow_summary(workflow)\n",
    "\n",
    "# Show basic metrics\n",
    "print(\"Overall Scores:\")\n",
    "print(f\"Math Accuracy Score: {accuracy['average_score']}\")\n",
    "print(f\"Trajectory Accuracy Score: {trajectory['average_score']}\")\n",
    "print(f\"Average Steps: {workflow_df['Steps'].mean()}\")\n",
    "print(f\"Average Tokens: {workflow_df['Tokens'].mean()}\")\n",
    "\n",
    "# Show accuracy results\n",
    "print(\"\\nMath Accuracy Results:\")\n",
    "display(accuracy_df)\n",
    "\n",
    "# Show workflow results\n",
    "print(\"\\nWorkflow Summary:\")\n",
    "display(workflow_df)\n",
    "\n",
    "# Count tool usage\n",
    "tools_used = {}\n",
    "for tools in workflow_df['Tools']:\n",
    "    for tool in tools.split(', '):\n",
    "        if tool:  # Skip empty strings\n",
    "            tools_used[tool] = tools_used.get(tool, 0) + 1\n",
    "\n",
    "print(\"\\nTools Used:\")\n",
    "display(pd.DataFrame(list(tools_used.items()), columns=['Tool', 'Count']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Observability\n",
    "\n",
    "Now that we can run our agent as a service, we need to monitor its performance and behavior. The NeMo Agent Toolkit provides comprehensive observability features that help us understand what's happening inside our agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observability and Profiling in the NeMo Agent Toolkit\n",
    "\n",
    "The NeMo Agent Toolkit offers comprehensive observability and profiling capabilities to monitor and optimize your workflows:\n",
    "\n",
    "1. **Telemetry Options**:\n",
    "   - **Logging**: Configure logs to console or file with different verbosity levels\n",
    "   - **Tracing**: Track the flow of requests through your system\n",
    "   - **Metrics**: Measure performance characteristics of your workflow\n",
    "\n",
    "2. **Profiling Tools**:\n",
    "   - **Token Usage Analysis**: Track and forecast token consumption\n",
    "   - **Latency Analysis**: Identify performance bottlenecks\n",
    "   - **Concurrency Analysis**: Understand parallel execution patterns\n",
    "\n",
    "3. **Tracing Providers**:\n",
    "   - **Phoenix Profiler**: A visualization tool by Arize AI for tracing and profiling\n",
    "   - **OpenTelemetry Collector**: Standard collector for observability data\n",
    "   - **Custom Providers**: Extensible system for custom telemetry exporters\n",
    "\n",
    "In this notebook, we'll use the **Phoenix Profiler** to visualize the execution of our agent and understand its performance characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding Observability in the NeMo Agent Toolkit\n",
    "\n",
    "The NeMo Agent Toolkit supports multiple observability options, including:\n",
    "\n",
    "- **Logging Providers**: Console logging and file-based logging with configurable verbosity levels\n",
    "- **Tracing Providers**: Phoenix Profiler, OpenTelemetry Collector, and custom providers\n",
    "- **Metrics Collection**: Performance measurements for optimization\n",
    "\n",
    "For this notebook, we'll use the **Phoenix Profiler** for tracing. Phoenix is developed by Arize AI (https://github.com/Arize-ai/phoenix) and provides detailed insights into your agent's execution, including:\n",
    "\n",
    "- Visual representation of the agent's reasoning process\n",
    "- Timing information for each step and tool call\n",
    "- Token usage statistics and bottleneck identification\n",
    "- Hierarchical view of nested function calls\n",
    "\n",
    "These features help with debugging, performance optimization, and understanding usage patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Updating Configuration for Observability\n",
    "\n",
    "Let's update our configuration file to enable observability features. We'll add a `telemetry` section to the `general` configuration that includes logging and tracing settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/math_tools/configs/observability_config.yml\n",
    "\n",
    "general:\n",
    "  use_uvloop: true\n",
    "  telemetry:\n",
    "    logging:\n",
    "        console:\n",
    "            _type: console\n",
    "            level: WARN\n",
    "    tracing:\n",
    "        phoenix:\n",
    "            _type: phoenix\n",
    "            endpoint: http://localhost:7007/v1/traces\n",
    "            project: math_tools_example\n",
    "\n",
    "functions:\n",
    "  calculator_exponent:\n",
    "    _type: calculator_exponent\n",
    "  calculator_modulus:\n",
    "    _type: calculator_modulus\n",
    "  calculator_square_root:\n",
    "    _type: calculator_square_root\n",
    "  calculator_add:\n",
    "    _type: calculator_add\n",
    "  current_datetime:\n",
    "    _type: current_datetime\n",
    "\n",
    "llms:\n",
    "  nim_llm:\n",
    "    _type: nim\n",
    "    model_name: meta/llama-3.1-70b-instruct\n",
    "    temperature: 0.0\n",
    "\n",
    "workflow:\n",
    "  _type: react_agent\n",
    "  tool_names:\n",
    "    - calculator_exponent\n",
    "    - calculator_modulus\n",
    "    - calculator_square_root\n",
    "    - calculator_add\n",
    "    - current_datetime\n",
    "  llm_name: nim_llm\n",
    "  verbose: true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "\n",
    "# Start the Phoenix server using Popen to gain direct control over the process\n",
    "# We also suppress the output by redirecting stdout and stderr\n",
    "phoenix_process = subprocess.Popen(\n",
    "    [\"phoenix\", \"serve\"],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL,\n",
    ")\n",
    "\n",
    "# Give Phoenix a moment to start up\n",
    "time.sleep(3)\n",
    "\n",
    "print(f\"Phoenix server started with PID: {phoenix_process.pid}\")\n",
    "print(\"You can access the Phoenix UI at: http://localhost:7007\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Running the NeMo Agent Toolkit with Observability Enabled\n",
    "\n",
    "Now let's run our agent with observability enabled. This will generate logs and traces that we can use to monitor and debug our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!nat run --config_file workflows/math_tools/configs/observability_config.yml --input \"What is the square root of the current hour plus 5?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have run the above (feel free to modify the query), look at Phoenix and you should see observability data flowing in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stopping the Phoenix Server\n",
    "\n",
    "Let's clean up for the next notebook by stopping the Phoenix server we started. In a production system, you would not stop Phoenix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the process object exists and is running\n",
    "if 'phoenix_process' in locals() and phoenix_process.poll() is None:\n",
    "    print(f\"Stopping Phoenix server with PID: {phoenix_process.pid}...\")\n",
    "    \n",
    "    # Send the termination signal to the process\n",
    "    phoenix_process.terminate()\n",
    "    \n",
    "    try:\n",
    "        # Wait for the process to terminate\n",
    "        phoenix_process.wait(timeout=5)\n",
    "        print(\"Phoenix server stopped successfully.\")\n",
    "    except subprocess.TimeoutExpired:\n",
    "        # If it doesn't terminate gracefully, force kill it\n",
    "        print(\"Server did not terminate gracefully. Forcing kill...\")\n",
    "        phoenix_process.kill()\n",
    "        phoenix_process.wait()\n",
    "        print(\"Phoenix server killed.\")\n",
    "    \n",
    "    # Clean up the variable\n",
    "    del phoenix_process\n",
    "else:\n",
    "    print(\"Phoenix server was not running or the process object was not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Introduction to Optimization\n\nNow that we've evaluated our agent and observed its behavior, the natural next step is to **optimize** it. The NeMo Agent Toolkit provides a built-in optimizer that can automatically tune your workflow's parameters to improve performance.\n\n### The Evaluate → Profile → Optimize Loop\n\nThe NeMo Agent Toolkit supports a powerful iterative development cycle:\n\n1. **Evaluate** — Measure your agent's accuracy and quality using `nat eval`\n2. **Profile** — Understand where time and tokens are spent (observability)\n3. **Optimize** — Automatically search for better parameter settings using `nat optimize`\n\n### How Does `nat optimize` Work?\n\nThe optimizer supports two complementary tuning modes:\n\n- **Numeric Optimization (Optuna)**: Searches over numeric hyperparameters like `temperature`, `max_tokens`, and `top_p` using Bayesian optimization. Fast and efficient for continuous parameters.\n\n- **Prompt Optimization (Genetic Algorithm)**: Evolves prompt templates using a genetic algorithm — generating, mutating, and recombining prompts across generations to find more effective phrasings.\n\nBoth modes can run together or independently, and support **multi-objective scoring** — optimizing for multiple metrics simultaneously (e.g., maximize accuracy while minimizing token usage).",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Creating an Optimizer Configuration\n\nLet's create a simple optimizer configuration for our math tools agent. We'll use **numeric-only optimization** with just 3 trials to keep it lightweight. The optimizer will tune `temperature` and `max_tokens` for our LLM while measuring accuracy.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "%%writefile workflows/math_tools/configs/optimize_config.yml\n\ngeneral:\n  use_uvloop: true\n\nfunctions:\n  calculator_exponent:\n    _type: calculator_exponent\n  calculator_modulus:\n    _type: calculator_modulus\n  calculator_square_root:\n    _type: calculator_square_root\n  calculator_add:\n    _type: calculator_add\n  current_datetime:\n    _type: current_datetime\n\nllms:\n  nim_llm:\n    _type: nim\n    model_name: meta/llama-3.1-70b-instruct\n    temperature: 0.0\n    search_space:\n      temperature:\n        low: 0.0\n        high: 0.5\n        step: 0.1\n      max_tokens:\n        low: 256\n        high: 1024\n        step: 256\n  eval_llm:\n    _type: nim\n    model_name: meta/llama-3.1-405b-instruct\n    temperature: 0.0\n    max_tokens: 1024\n\nworkflow:\n  _type: react_agent\n  tool_names:\n    - calculator_exponent\n    - calculator_modulus\n    - calculator_square_root\n    - calculator_add\n    - current_datetime\n  llm_name: nim_llm\n  verbose: true\n\neval:\n  general:\n    output_dir: ./math_tools_optimize/\n    dataset:\n      _type: json\n      file_path: workflows/math_tools/data/comprehensive_eval.json\n  evaluators:\n    math_accuracy:\n      _type: ragas\n      metric: AnswerAccuracy\n      llm_name: eval_llm\n\noptimizer:\n  output_path: ./math_tools_optimize/\n  reps_per_param_set: 1\n  eval_metrics:\n    accuracy:\n      evaluator_name: math_accuracy\n      direction: maximize\n  numeric:\n    enabled: true\n    n_trials: 3\n  prompt:\n    enabled: false",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Running the Optimizer\n\nLet's run the optimizer. It will execute 3 trials with different parameter combinations and score each one using our accuracy evaluator.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "!nat optimize --config_file=workflows/math_tools/configs/optimize_config.yml",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Inspecting Optimization Results\n\nAfter the optimizer completes, it writes several artifacts to the output directory. Let's look at the trial results and the best configuration found.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pathlib import Path\n\noptimizer_output = Path(\"./math_tools_optimize/\")\n\n# Show trial results if available\ntrials_csv = optimizer_output / \"trials_dataframe_params.csv\"\nif trials_csv.exists():\n    trials_df = pd.read_csv(trials_csv)\n    print(\"Optimization Trials:\")\n    display(trials_df)\nelse:\n    print(\"No trials CSV found yet — run the optimizer cell above first.\")\n\n# Show the optimized config if available\noptimized_config = optimizer_output / \"optimized_config.yml\"\nif optimized_config.exists():\n    print(\"\\nOptimized Configuration:\")\n    print(optimized_config.read_text())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, we've explored how to evaluate, observe, and optimize our NeMo Agent Toolkit workflow. We've learned how to:\n\n1. Create comprehensive evaluation datasets with test cases for different capabilities\n2. Configure and run evaluations using RAGAS and Trajectory evaluators\n3. Analyze evaluation results to understand agent performance\n4. Handle time-based queries and dynamic responses in evaluations\n5. Set up observability features using Phoenix Profiler by Arize AI for monitoring and debugging\n6. Run the NAT optimizer to automatically tune LLM hyperparameters\n7. Inspect optimization trial results and the best configuration found\n\nThese techniques form the **evaluate → observe → optimize** loop that helps ensure your agents perform reliably and efficiently. For a deeper exploration of these topics using a production-grade workflow, check out the [Deep Dive](deep-dive/) notebooks."
  },
  {
   "cell_type": "markdown",
   "source": "### Going Deeper\n\n> This section introduced the basics of NAT's optimization capabilities using a simple numeric search over LLM parameters. For a **production-grade deep dive** — including custom evaluators, prompt genetic algorithm optimization, Pareto trade-offs, and multi-objective tuning on a real-world email phishing workflow — see the **[Deep Dive](deep-dive/)** notebooks in the `deep-dive/` directory.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
